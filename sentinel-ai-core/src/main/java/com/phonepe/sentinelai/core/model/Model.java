package com.phonepe.sentinelai.core.model;

import com.phonepe.sentinelai.core.agent.*;
import com.phonepe.sentinelai.core.agentmessages.AgentMessage;
import com.phonepe.sentinelai.core.tools.ExecutableTool;
import org.apache.commons.lang3.NotImplementedException;

import java.util.Collection;
import java.util.List;
import java.util.Map;
import java.util.concurrent.CompletableFuture;
import java.util.function.Consumer;

/**
 * Abstract representation for a LLM model
 */
public interface Model {

    /**
     * Process messages and generate structured output. Supports both structured and tool based outputs.
     * Response will consist of a JsonNode in the form of a Map<String, Object> where the keys are the
     * output names and the values are the generated outputs.
     *
     * @param context The context of the model run, which includes metadata about the agent and the request.
     * @param outputDefinitions The definitions of the outputs that the model should generate.
     * @param oldMessages The previous messages in the conversation, which can be used to provide context for the model.
     * @param tools A map of tool names to executable tools that can be used by the model to generate outputs.
     * @param toolRunner The tool runner that will execute the tools when needed.
     * @return A CompletableFuture that will complete with the generated Model
     */
    CompletableFuture<ModelOutput> compute(
            ModelRunContext context,
            Collection<ModelOutputDefinition> outputDefinitions,
            List<AgentMessage> oldMessages, Map<String, ExecutableTool> tools,
            ToolRunner toolRunner);

    /**
     * Streams output from the model based on the provided context and messages. Supports tool calls.
     * Stream handler will be called with the output as it is generated by the model. Tool call fragments etc will
     * not be streamed out.
     *
     * @param context       The context of the model run, which includes metadata about the agent and the request.
     * @param outputDefinitions The definitions of the outputs that the model should generate.
     * @param oldMessages   The previous messages in the conversation, which can be used to provide context for the
     *                      model.
     * @param tools         List of available tools that can be used by the model to generate outputs.
     * @param toolRunner    A runner for the tools that will execute the tools when needed.
     * @param streamHandler A consumer that will handle the streamed output from the model. It will be called with
     *                      byte arrays as the output is generated.
     * @return A CompletableFuture that will complete with the generated ModelOutput when the streaming is done.
     */
    default CompletableFuture<ModelOutput> stream(
            ModelRunContext context,
            Collection<ModelOutputDefinition> outputDefinitions,
            List<AgentMessage> oldMessages,
            Map<String, ExecutableTool> tools,
            ToolRunner toolRunner,
            Consumer<byte[]> streamHandler) {
        throw new NotImplementedException();
    }

    /**
     * Streams text output from the model based on the provided context and messages. Supports tool calls.
     * Stream handler will be called with the output as it is generated by the model. Tool call fragments etc will
     * not be streamed out.
     *
     * @param context       The context of the model run, which includes metadata about the agent and the request.
     * @param oldMessages   The previous messages in the conversation, which can be used to provide context for the
     *                      model.
     * @param tools         List of available tools that can be used by the model to generate outputs.
     * @param toolRunner    A runner for the tools that will execute the tools when needed.
     * @param streamHandler A consumer that will handle the streamed output from the model. It will be called with
     *                      byte arrays as the output is generated.
     * @return A CompletableFuture that will complete with the generated ModelOutput when the streaming is done.
     */
    default CompletableFuture<ModelOutput> streamText(
            ModelRunContext context,
            List<AgentMessage> oldMessages,
            Map<String, ExecutableTool> tools,
            ToolRunner toolRunner,
            Consumer<byte[]> streamHandler) {
        throw new NotImplementedException();
    }
}
